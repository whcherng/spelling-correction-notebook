{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original statement with Errors Highlighted:\n",
      "Eighteen participants revealed that they shared information from the Extranet with others outside of the comitee. The information they shared most often included provincial directives and local *informasion* such as Hamilton medical *advisors* and Steering Committee minutes\n",
      "\n",
      "Correction Suggestions:\n",
      "\n",
      "Incorrect word: informasion\n",
      "Top 5 suggestions:\n",
      "1. information (Prob: 0.000465, Distance: 1, Score: 0.000325)\n",
      "\n",
      "Incorrect word: advisors\n",
      "Top 5 suggestions:\n",
      "1. advisories (Prob: 0.000264, Distance: 2, Score: 0.000129)\n",
      "2. advisory (Prob: 0.000089, Distance: 1, Score: 0.000062)\n",
      "3. advisor (Prob: 0.000088, Distance: 1, Score: 0.000062)\n",
      "\n",
      "Corrected statements: \n",
      "Eighteen participants revealed that they shared information from the Extranet with others outside of the comitee. The information they shared most often included provincial directives and local information such as Hamilton medical advisories and Steering Committee minutes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add in tokenize at open_file, change vocabulary into correct_word, build_models change to build_ngram_models, candidate/s to suggestion/s  \n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from nltk import edit_distance, bigrams\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "\n",
    "# Open and clean corpus\n",
    "def open_file(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        text = f.read().lower()\n",
    "    \n",
    "    # Separate punctuation, numbers and spaces/tabs from words and clean it\n",
    "    text = re.sub(r\"([.,!?'])\", r\" \\1 \", text)  # Add spaces around punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z'.,!? ]\", \" \", text)  # Remove numbers/symbols\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Collapse multiple spaces\n",
    "\n",
    "    tokenizer = Tokenizer(nlp.vocab)\n",
    "    tokens = tokenizer(text)\n",
    "    token_text = [token.text for token in tokens]\n",
    "\n",
    "    return token_text\n",
    "\n",
    "# Build language models\n",
    "def build_ngram_models(words):\n",
    "    unigram_fd = FreqDist(words)\n",
    "    bigram_cfd = ConditionalFreqDist()\n",
    "    for prev, curr in bigrams(words):\n",
    "        bigram_cfd[prev][curr] += 1\n",
    "    return unigram_fd, bigram_cfd\n",
    "\n",
    "# Generate suggestions with prioritization\n",
    "def generate_suggestions(word, correct_word, max_distance=2):\n",
    "    suggestions = []\n",
    "    for suggestion in correct_word:\n",
    "        distance = edit_distance(word, suggestion, transpositions=True)\n",
    "        if distance <= max_distance:\n",
    "            suggestions.append((suggestion, distance))\n",
    "    # Sort by edit distance first\n",
    "    return [c[0] for c in sorted(suggestions, key=lambda x: x[1])][:100]\n",
    "\n",
    "# Calculate the probability with corpus validation\n",
    "def calc_prob(word, prev_word, unigram_fd, bigram_cfd, vocab_size):\n",
    "    # Check if word exists in corpus file\n",
    "    if word not in unigram_fd:\n",
    "        return 0.0  # Treat OOV words as invalid\n",
    "    \n",
    "    # Unigram probability\n",
    "    unigram_prob = unigram_fd[word] / unigram_fd.N()\n",
    "    \n",
    "    # Bigram probability if context exists\n",
    "    if prev_word and prev_word in bigram_cfd:\n",
    "        bigram_prob = (bigram_cfd[prev_word][word] + 1) / (unigram_fd[prev_word] + vocab_size)\n",
    "    else:\n",
    "        bigram_prob = unigram_prob\n",
    "    \n",
    "    return 0.1 * unigram_prob + 0.9 * bigram_prob\n",
    "\n",
    "# Improve correction logic\n",
    "def correct_spell(statement, unigram_fd, bigram_cfd, vocab_size, correct_word):\n",
    "    words = statement.lower().split()\n",
    "    results = []\n",
    "    \n",
    "    # Use top 10% most frequent words as threshold\n",
    "    common_words = {word for word, _ in unigram_fd.most_common(int(len(unigram_fd)*0.3))} # 30% top freq word\n",
    "    #common_words = {word for word, _ in unigram_fd.most_common(int(len(unigram_fd)*0.1))}\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        # Skip punctuation-only tokens\n",
    "        if re.fullmatch(r\"[.,!?']+\", word):\n",
    "            continue\n",
    "            \n",
    "        # Get previous word context\n",
    "        prev_word = words[i-1] if i > 0 and words[i-1] in unigram_fd else None\n",
    "        \n",
    "        # Generate suggestions if:\n",
    "        # 1. Word is OOV (not in correct_word), or\n",
    "        # 2. Word is rare (not in top 10% frequent words)\n",
    "        if word not in unigram_fd or word not in common_words:\n",
    "            suggestions = generate_suggestions(word, correct_word)\n",
    "            if suggestions:\n",
    "                scored = []\n",
    "                for suggestion in suggestions:\n",
    "                    # Skip suggestion if same as original\n",
    "                    if suggestion == word:\n",
    "                        continue\n",
    "                        \n",
    "                    edit_dist = edit_distance(word, suggestion, transpositions=True)\n",
    "                    suggestion_prob = calc_prob(suggestion, prev_word, unigram_fd, bigram_cfd, vocab_size)\n",
    "                    error_prob = 0.7 ** edit_dist  # Less aggressive error model\n",
    "                    score = suggestion_prob * error_prob\n",
    "                    scored.append((suggestion, suggestion_prob, edit_dist, score))\n",
    "                \n",
    "                if scored:\n",
    "                    # Sort by score and distance\n",
    "                    top_suggestions = sorted(scored, key=lambda x: (-x[3], x[2]))[:5]\n",
    "                    #top_suggestions = sorted(scored, key=lambda x: (-x[3], x[2]))[:5]\n",
    "                    results.append((word, top_suggestions))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Format and display results\n",
    "def display_results(statement, correction_results):\n",
    "    words = statement.split()\n",
    "    output = []\n",
    "    corrected_statements = []\n",
    "    \n",
    "    # Create mapping of incorrect words to their corrections\n",
    "    #corrections_map = {res[0]: res[1] for res in correction_results}\n",
    "    corrections_map = {res[0]: res[1][0][0] for res in correction_results}\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() in corrections_map:\n",
    "            output.append(f\"*{word}*\")  # Highlight incorrect word\n",
    "            corrected_statements.append(corrections_map[word.lower()]) # To add corrected word\n",
    "        else:\n",
    "            output.append(word)\n",
    "            corrected_statements.append(word)\n",
    "    \n",
    "    print(\"\\nOriginal statement with Errors Highlighted:\")\n",
    "    print(\" \".join(output))\n",
    "    \n",
    "    print(\"\\nCorrection Suggestions:\")\n",
    "    for incorrect_word, suggestions in correction_results:\n",
    "        print(f\"\\nIncorrect word: {incorrect_word}\")\n",
    "        print(\"Top 5 suggestions:\")\n",
    "        for i, (word, prob, distance, score) in enumerate(suggestions, 1):\n",
    "            print(f\"{i}. {word} (Prob: {prob:.6f}, Distance: {distance}, Score: {score:.6f})\")\n",
    "\n",
    "    print(\"\\nCorrected statements: \")\n",
    "    print(\" \".join(corrected_statements))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load corpus and build models\n",
    "    corpus_file = 'D:/MASTER/NLP/Assignment/output-COVID19.txt'  # Update with your corpus path\n",
    "    words = open_file(corpus_file)\n",
    "    unigram_fd, bigram_cfd = build_ngram_models(words)\n",
    "    vocab_size = len(unigram_fd)\n",
    "    correct_word = list(unigram_fd.keys())\n",
    "    \n",
    "    # Example statement\n",
    "    #statement = \"In oder to demonstrate that Mat0-RNA3 can be used as an effective tol in recombination studies, we apply i to examine the recombination activity of too specific sequences derived frm the HCV genome.\"\n",
    "    statement = input(\"Enter your statement below to have spelling check: \\n\")\n",
    "\n",
    "    # Get correction results\n",
    "    results = correct_spell(statement, unigram_fd, bigram_cfd, vocab_size, correct_word)\n",
    "    \n",
    "    # Display results\n",
    "    display_results(statement, results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
