{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.util import bigrams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def openfile(path): #open and read the the corpus file\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        txtfile = file.read()\n",
    "\n",
    "    return txtfile\n",
    "\n",
    "def clean_corpus(txtfile,outputpath): #clean corpus file, lemmatization and tokenize the token into output file to view\n",
    "    doc1 = nlp(txtfile)\n",
    "    correct_word = [token.lemma_.lower() for token in doc1 if not token.is_punct]\n",
    "\n",
    "    with open(outputpath, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"\\n\".join(correct_word))\n",
    "    \n",
    "    print(f\"Data is cleaned and save to {outputpath}\")\n",
    "    print(f\"Sample of correct words: {correct_word[:10]}\")  # Debug print\n",
    "\n",
    "    return correct_word\n",
    "\n",
    "def bigrams_model(correct_word):\n",
    "    bigram_list = list(bigrams(correct_word)) #generate bigrams from the corpus\n",
    "    bigram_count = Counter(bigram_list) # to get the frequency of bigram\n",
    "    unigram_count = Counter(correct_word) # get the unigram frequency \n",
    "    print(f\"bigram count is is here\")\n",
    "    print(correct_word)\n",
    "    print(\"Bigram list as below:\")\n",
    "    print(bigram_list)\n",
    "    print(\"bigram count as below:\")\n",
    "    #print(bigram_count)\n",
    "\n",
    "    bigram_probability = defaultdict(float)\n",
    "\n",
    "    for bigram1, count in bigram_count.items():\n",
    "        bigram_probability[bigram1] = count / unigram_count[bigram1[0]]\n",
    "    return bigram_probability\n",
    "\n",
    "def probability_statements(statements, bigram_probability):\n",
    "    prob_statements = []\n",
    "    for statement in statements:\n",
    "        token1 = statement.split()\n",
    "        tot_prob = 1.00\n",
    "        range_stop = len(token1) - 1\n",
    "\n",
    "\n",
    "        for i in range(range_stop):\n",
    "            bigram = (token1[i], token1[i + 1]) # taking current word and next word\n",
    "            tot_prob *= bigram_probability.get(bigram, 1e-6) # using a small default probability for the unseen bigrams\n",
    "\n",
    "        prob_statements.append((statement, tot_prob))\n",
    "    prob_statements = sorted(prob_statements, key=lambda x:x[1], reverse=True)\n",
    "        \n",
    "    return [statement for statement, _ in prob_statements]\n",
    "\n",
    "def word_suggestion(wrong_word, correct_word, substitution_cost=2, topcount=5): \n",
    "    suggest = []\n",
    "    for word in correct_word:\n",
    "        distance = edit_distance(wrong_word, word)\n",
    "        if distance <= substitution_cost:\n",
    "            suggest.append((word, distance))\n",
    "\n",
    "    suggest.sort(key=lambda x: x[1])\n",
    "    return suggest[:topcount]\n",
    "        \n",
    "def statement_suggestion(input_statement,suggestion, count=0, statements=[]):\n",
    "    corrected_text=[]\n",
    "    \n",
    "    if count == len(input_statement):\n",
    "        return [\" \".join(statements)]\n",
    "    \n",
    "    token = input_statement[count]\n",
    "\n",
    "    # if there wrong spelling, replace on the possible correction, else remain as it is.\n",
    "    if token in suggestion: \n",
    "        for i in suggestion[token]:\n",
    "            corrected_text.extend(statement_suggestion(input_statement, suggestion, count + 1, statements + [i[0]]))\n",
    "    else:\n",
    "        corrected_text.extend(statement_suggestion(input_statement, suggestion, count + 1, statements + [token]))\n",
    "\n",
    "    return corrected_text    \n",
    "\n",
    "def main():\n",
    "    path = 'D:/MASTER/NLP/Assignment/output-COVID19.txt'\n",
    "    outputpath = 'D:/MASTER/NLP/Assignment/output-cleaned-no_stop.txt'\n",
    "    txtfile = openfile(path)\n",
    "\n",
    "    correct_word = clean_corpus(txtfile, outputpath)\n",
    "    print(f\"corrrect wordssss are {correct_word}\")\n",
    "\n",
    "    bigram_prob = bigrams_model(correct_word)\n",
    "    print (f\"output is {bigram_prob}\")\n",
    "\n",
    "    statement1 = \"Finnaly commbination is there \"\n",
    "    #statement1 = input(\"Enter your statement below to have spelling check: \")\n",
    "\n",
    "    words = statement1.split()\n",
    "\n",
    "    need_correction = {}\n",
    "    for token in words:\n",
    "        if token not in correct_word:\n",
    "            suggest = word_suggestion(token, correct_word)\n",
    "\n",
    "            if suggest:\n",
    "                need_correction[token] = suggest\n",
    "                print(f\"Wrong words: '{token}' \")\n",
    "                print(f\"Suggestion is : {[j[0] for j in suggest]}\")\n",
    "            else:\n",
    "                print(f\"No suggestions found for '{token}'\")  # Debug print\n",
    "\n",
    "    if need_correction:\n",
    "        new_statements = statement_suggestion(words, need_correction)\n",
    "        chk_rank_statement = probability_statements(new_statements, bigram_prob)[:5]\n",
    "\n",
    "        print(\"Potential correct statement: \")\n",
    "        for i, statement2 in enumerate(chk_rank_statement, 1):\n",
    "            print(f\"{i}. {statement2}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No errors found.\")\n",
    "\n",
    "#    for word, distance in suggest:\n",
    " #       print(suggest,distance)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
